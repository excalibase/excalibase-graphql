name: Enterprise Benchmark Tests

on:
  workflow_run:
    workflows: ["Continuous Integration"]
    types:
      - completed
  workflow_dispatch: # Allow manual triggering
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'modules/**'
      - 'scripts/benchmark-initdb.sql'
      - 'scripts/e2e-benchmark.sh'
      - 'docker-compose.benchmark.yml'
      - 'Makefile'

jobs:
  enterprise-benchmark:
    name: Enterprise-Scale Performance Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request' || github.event.workflow_run.conclusion == 'success'
    
    permissions:
      contents: read
      pull-requests: write  # Required for posting comments
      actions: read
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'corretto'
        
    - name: Cache Maven dependencies
      uses: actions/cache@v4
      with:
        path: ~/.m2/repository
        key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}
        restore-keys: |
          ${{ runner.os }}-maven-
          
    - name: Download JAR artifacts (if from workflow_run)
      if: github.event_name == 'workflow_run'
      uses: actions/download-artifact@v4
      with:
        name: jar-artifacts
        path: modules/excalibase-graphql-api/target/
        github-token: ${{ secrets.GITHUB_TOKEN }}
        run-id: ${{ github.event.workflow_run.id }}
        
    - name: Install dependencies
      run: |
        # Install jq if missing
        if ! command -v jq >/dev/null 2>&1; then
          sudo apt-get update && sudo apt-get install -y jq
        fi
        
        # Install docker-compose if missing
        if ! command -v docker-compose >/dev/null 2>&1; then
          sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
          docker-compose --version
        fi
        
    - name: Run Enterprise Benchmark Tests  
      id: benchmark
      run: |
        echo "🏢 Starting Enterprise-Scale Benchmark Tests..."
        
        # Create output file for results
        BENCHMARK_OUTPUT="benchmark-results.txt"
        BENCHMARK_JSON="benchmark-results.json"
        
        # Run benchmark and capture output
        echo "🏃 Starting benchmark execution..."
        set -o pipefail  # Fail if any command in pipeline fails
        if make ci-benchmark 2>&1 | tee "$BENCHMARK_OUTPUT"; then
          echo "✅ Enterprise benchmark tests completed successfully"
          echo "benchmark_success=true" >> $GITHUB_OUTPUT
          
          # Debug: Show captured output for troubleshooting
          echo "📋 Captured benchmark output (last 20 lines):"
          tail -20 "$BENCHMARK_OUTPUT"
        else
          echo "❌ Enterprise benchmark tests failed"
          echo "benchmark_success=false" >> $GITHUB_OUTPUT
          
          # Debug: Show error output
          echo "📋 Error output (last 50 lines):"
          tail -50 "$BENCHMARK_OUTPUT"
          
          # Additional debug: Show docker-compose logs
          echo "📋 Docker service logs:"
          docker-compose -f docker-compose.benchmark.yml -p excalibase-benchmark logs --tail=100
          
          exit 1
        fi
        
        # Extract key metrics from output (matching actual benchmark script format with color codes)
        # Format: ${GREEN}[SUCCESS]${NC} ✅ Test name: XXms (threshold: YYms) or XXms, additional info (threshold: YYms)
        # Need to handle ANSI color codes in the output
        SCHEMA_TIME=$(grep -o "SUCCESS.*✅ Schema introspection: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        MILLION_QUERY_TIME=$(grep -o "SUCCESS.*✅ Million-record query: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        JOIN_TIME=$(grep -o "SUCCESS.*✅ Massive JOIN query: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        ENHANCED_TIME=$(grep -o "SUCCESS.*✅ Enhanced types at scale: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        CONCURRENT_TIME=$(grep -o "SUCCESS.*✅ 50 concurrent requests: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        CONCURRENT_AVG=$(grep -o "SUCCESS.*✅ 50 concurrent requests: [0-9]*ms ([0-9]*ms avg)" "$BENCHMARK_OUTPUT" | grep -o "([0-9]*ms avg)" | grep -o "[0-9]*" | head -1 || echo "N/A")
        LARGE_RESULT_TIME=$(grep -o "SUCCESS.*✅ Large result set query: [0-9]*ms" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        MEMORY_USAGE=$(grep -o "🐳 Container Memory Usage: [0-9]*MB" "$BENCHMARK_OUTPUT" | grep -o "[0-9]*" | head -1 || echo "N/A")
        
        # Debug: Show extracted metrics
        echo "📊 Extracted metrics:"
        echo "  - Schema introspection: ${SCHEMA_TIME}ms"
        echo "  - Million-record query: ${MILLION_QUERY_TIME}ms"
        echo "  - Massive JOIN: ${JOIN_TIME}ms"
        echo "  - Enhanced types: ${ENHANCED_TIME}ms"
        echo "  - Concurrent requests: ${CONCURRENT_TIME}ms (${CONCURRENT_AVG}ms avg)"
        echo "  - Large result set: ${LARGE_RESULT_TIME}ms"
        echo "  - Memory usage: ${MEMORY_USAGE}MB"
        
        # Convert N/A to null for JSON
        SCHEMA_TIME_JSON=$([ "$SCHEMA_TIME" = "N/A" ] && echo "null" || echo "$SCHEMA_TIME")
        MILLION_QUERY_TIME_JSON=$([ "$MILLION_QUERY_TIME" = "N/A" ] && echo "null" || echo "$MILLION_QUERY_TIME")
        JOIN_TIME_JSON=$([ "$JOIN_TIME" = "N/A" ] && echo "null" || echo "$JOIN_TIME")
        ENHANCED_TIME_JSON=$([ "$ENHANCED_TIME" = "N/A" ] && echo "null" || echo "$ENHANCED_TIME")
        CONCURRENT_TIME_JSON=$([ "$CONCURRENT_TIME" = "N/A" ] && echo "null" || echo "$CONCURRENT_TIME")
        CONCURRENT_AVG_JSON=$([ "$CONCURRENT_AVG" = "N/A" ] && echo "null" || echo "$CONCURRENT_AVG")
        LARGE_RESULT_TIME_JSON=$([ "$LARGE_RESULT_TIME" = "N/A" ] && echo "null" || echo "$LARGE_RESULT_TIME")
        MEMORY_USAGE_JSON=$([ "$MEMORY_USAGE" = "N/A" ] && echo "null" || echo "$MEMORY_USAGE")
        
        # Create JSON output for comment
        cat > "$BENCHMARK_JSON" << EOF
        {
          "success": true,
          "timestamp": "$(date -u +"%Y-%m-%d %H:%M:%S UTC")",
          "metrics": {
            "schema_introspection_ms": $SCHEMA_TIME_JSON,
            "million_record_query_ms": $MILLION_QUERY_TIME_JSON,
            "massive_join_ms": $JOIN_TIME_JSON,
            "enhanced_types_ms": $ENHANCED_TIME_JSON,
            "concurrent_requests_ms": $CONCURRENT_TIME_JSON,
            "concurrent_requests_avg_ms": $CONCURRENT_AVG_JSON,
            "large_result_set_ms": $LARGE_RESULT_TIME_JSON,
            "memory_usage_mb": $MEMORY_USAGE_JSON
          },
          "dataset": {
            "companies": 10000,
            "departments": 100000,
            "employees": 1000000,
            "projects": 50000,
            "time_entries": 5000000,
            "audit_logs": 10000000,
            "total_records": 16670000
          }
        }
        EOF
        
        # Set outputs for comment generation
        echo "schema_time=$SCHEMA_TIME" >> $GITHUB_OUTPUT
        echo "million_query_time=$MILLION_QUERY_TIME" >> $GITHUB_OUTPUT
        echo "join_time=$JOIN_TIME" >> $GITHUB_OUTPUT
        echo "enhanced_time=$ENHANCED_TIME" >> $GITHUB_OUTPUT
        echo "concurrent_time=$CONCURRENT_TIME" >> $GITHUB_OUTPUT
        echo "concurrent_avg=$CONCURRENT_AVG" >> $GITHUB_OUTPUT
        echo "large_result_time=$LARGE_RESULT_TIME" >> $GITHUB_OUTPUT
        echo "memory_usage=$MEMORY_USAGE" >> $GITHUB_OUTPUT
        
    - name: Generate Performance Report
      if: always()
      id: report
      run: |
        if [ "${{ steps.benchmark.outputs.benchmark_success }}" = "true" ]; then
          # Success report
          cat > benchmark-comment.md << 'EOF'
        ## 🏢 Enterprise Benchmark Results
        
        ### ✅ **PERFORMANCE TESTS PASSED**
        
        **📊 Benchmark Results** (`${{ steps.benchmark.outputs.schema_time || 'N/A' }}`ms avg)
        
        | Test Category | Response Time | Performance Rating |
        |---------------|---------------|-------------------|
        | 🔍 Schema Introspection (50+ tables) | `${{ steps.benchmark.outputs.schema_time || 'N/A' }}`ms | ${{ steps.benchmark.outputs.schema_time && steps.benchmark.outputs.schema_time < 1000 && '🟢 Excellent' || '🟡 Good' }} |
        | ⚡ Million-Record Query | `${{ steps.benchmark.outputs.million_query_time || 'N/A' }}`ms | ${{ steps.benchmark.outputs.million_query_time && steps.benchmark.outputs.million_query_time < 100 && '🟢 Excellent' || '🟡 Good' }} |
        | 🔗 Massive JOINs (5M+ records) | `${{ steps.benchmark.outputs.join_time || 'N/A' }}`ms | ${{ steps.benchmark.outputs.join_time && steps.benchmark.outputs.join_time < 100 && '🟢 Excellent' || '🟡 Good' }} |
        | 🎯 Enhanced Types (JSON/Arrays) | `${{ steps.benchmark.outputs.enhanced_time || 'N/A' }}`ms | ${{ steps.benchmark.outputs.enhanced_time && steps.benchmark.outputs.enhanced_time < 100 && '🟢 Excellent' || '🟡 Good' }} |
        | 🚀 50 Concurrent Requests | `${{ steps.benchmark.outputs.concurrent_time || 'N/A' }}`ms (`${{ steps.benchmark.outputs.concurrent_avg || 'N/A' }}`ms avg) | ${{ steps.benchmark.outputs.concurrent_time && steps.benchmark.outputs.concurrent_time < 2000 && '🟢 Excellent' || '🟡 Good' }} |
        | 📊 Large Result Sets (10M logs) | `${{ steps.benchmark.outputs.large_result_time || 'N/A' }}`ms | ${{ steps.benchmark.outputs.large_result_time && steps.benchmark.outputs.large_result_time < 200 && '🟢 Excellent' || '🟡 Good' }} |
        
        **💾 Resource Efficiency**
        - **Memory Usage**: `${{ steps.benchmark.outputs.memory_usage || 'N/A' }}`MB
        - **Dataset Scale**: 16.7M+ records across 50+ tables
        - **Efficiency Rating**: ${{ steps.benchmark.outputs.memory_usage && steps.benchmark.outputs.memory_usage < 500 && '🏆 Ultra-Efficient' || '⭐ Efficient' }}
        
        **🎯 Enterprise Readiness Assessment**
        - ✅ **Scale**: Handles 16.7M+ records
        - ✅ **Speed**: Sub-100ms response times
        - ✅ **Memory**: Efficient resource utilization
        - ✅ **Concurrency**: 50 simultaneous users supported
        - ✅ **Types**: Full PostgreSQL enhanced type support
        
        ---
        📝 **Benchmark Details**: All tests passed with enterprise-grade performance metrics.
        
        <details>
        <summary>📋 Complete Test Dataset</summary>
        
        - **Companies**: 10,000 records
        - **Departments**: 100,000 records  
        - **Employees**: 1,000,000 records
        - **Projects**: 50,000 records
        - **Time Entries**: 5,000,000 records
        - **Audit Logs**: 10,000,000 records
        - **Total Dataset**: 16.67M+ records
        
        </details>
        EOF
        else
          # Failure report
          cat > benchmark-comment.md << 'EOF'
        ## 🏢 Enterprise Benchmark Results
        
        ### ❌ **PERFORMANCE TESTS FAILED**
        
        The enterprise benchmark tests encountered issues. Please check the logs for details.
        
        **🔧 Next Steps:**
        1. Review the benchmark logs in the Actions tab
        2. Check for resource constraints or timeout issues
        3. Verify database initialization completed successfully
        4. Ensure all test services started properly
        
        ---
        📝 **Note**: Enterprise benchmarks require significant resources (16.7M+ records). Failure may indicate infrastructure limitations rather than code issues.
        EOF
        fi
        
        echo "report_generated=true" >> $GITHUB_OUTPUT
        
    - name: Post Benchmark Results to PR
      if: github.event_name == 'pull_request' && steps.report.outputs.report_generated == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read the generated comment
          const comment = fs.readFileSync('benchmark-comment.md', 'utf8');
          
          // Find existing benchmark comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.payload.pull_request.number
          });
          
          const existingComment = comments.find(c => 
            c.body.includes('🏢 Enterprise Benchmark Results')
          );
          
          if (existingComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: comment
            });
            console.log('✅ Updated existing benchmark comment');
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body: comment
            });
            console.log('✅ Created new benchmark comment');
          }
          
    - name: Upload benchmark results as artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: enterprise-benchmark-results
        path: |
          benchmark-results.txt
          benchmark-results.json
          benchmark-comment.md
        retention-days: 7
        if-no-files-found: ignore
        
    - name: Collect logs on failure
      if: failure()
      run: |
        echo "📋 Collecting enterprise benchmark logs..."
        if docker-compose -f docker-compose.benchmark.yml -p excalibase-benchmark logs > benchmark-logs.txt 2>&1; then
          echo "✅ Logs collected"
        else
          echo "⚠️ No logs available or services not running"
        fi
        
    - name: Upload failure logs
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: enterprise-benchmark-failure-logs
        path: |
          benchmark-logs.txt
        retention-days: 3
        if-no-files-found: ignore
        
    - name: Summary
      if: always()
      run: |
        if [ "${{ steps.benchmark.outputs.benchmark_success }}" = "true" ]; then
          echo "🎉 Enterprise benchmarks completed successfully!"
          echo "📊 Check the PR comment for detailed performance metrics."
        else
          echo "❌ Enterprise benchmarks failed. Check logs for details."
          exit 1
        fi